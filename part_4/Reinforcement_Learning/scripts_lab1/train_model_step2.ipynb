{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Второй этап, обучение на победу***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выполнил Кузин Мирослав**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongala import Kalah\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Болваны**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Бот простой, рандомно выбирает не нулевые элементы\n",
    "def do_simple_bot_step(state: torch.Tensor) -> int:\n",
    "    nonzero_state_indexs = torch.nonzero(state[:6]).flatten()\n",
    "    rez = nonzero_state_indexs[torch.randint(0, len(nonzero_state_indexs), (1,))[0]]\n",
    "    return rez + 1\n",
    "\n",
    "do_simple_bot_step(torch.Tensor([1, 0, 1, 0, 0, 0, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model_non_zero_top_tryed3.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Настройка обучения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def loss_func(probs: torch.Tensor, action: int, m: Categorical, R: int, gamma: int):\n",
    "    alpha = 0.9\n",
    "    # print(Categorical(probs).log_prob(action))\n",
    "    return -alpha*R*m.log_prob(action)\n",
    "\n",
    "loss = loss_func\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-4, amsgrad=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 6, 6, 6, 6, 0, 0, 6, 6, 6, 6, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.6487e-03, 9.9037e-01, 7.9768e-03, 3.3244e-06, 3.4061e-06, 7.6303e-08],\n",
       "       dtype=torch.float64, grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка работы модели\n",
    "game = Kalah()\n",
    "model.to(device)\n",
    "model.to(torch.double)\n",
    "\n",
    "print(game.get_general_state())\n",
    "model(game.get_general_state().to(torch.double))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Награды и штрафы*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Второй этап обучения\n",
    "winner_reward_stage_2 = 20\n",
    "loser_reward_stage_2 = -20\n",
    "draw_reward_stage_2 = -10\n",
    "bad_step_reward_stage_2 = -30 # общий\n",
    "good_step_reward_stage_2 = 1\n",
    "good_step_captured_reward_stage_2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(313.5733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-6923.2118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1772.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-879.5154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-3687.4191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-1815.1157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-247.3717, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(943.0535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1241.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3172.2925, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1491.9932, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-1485.2762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3160.8632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1433.9496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(756.9171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-121.8381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2007.4755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1952.7002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1878.4710, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4022.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1087.4233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3737.3700, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1205.2907, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-1378.9989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1796.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1587.3246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(334.3237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1075.1193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-478.9849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10317.0532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(466.8083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-4805.8349, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1916.0462, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2888.0533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(374.4404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2012.4225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2957.0804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2849.1135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3109.6176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-729.1519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2001.5901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4490.2768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(630.2016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-861.6481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3387.4250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(518.4807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2050.7745, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(336.3783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(589.2674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-2419.2881, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "game = Kalah()\n",
    "episodes_count = 10000\n",
    "neuronet_walker_queue = 1\n",
    "count_win_neuronet = 0\n",
    "count_choisen_zero = 0\n",
    "gamma = 2\n",
    "loss_story = []\n",
    "check_optim = False\n",
    "\n",
    "for episode in range(0, episodes_count):\n",
    "    game.set_new_game()\n",
    "    tr = []\n",
    "    if episode % 2:\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "        neuronet_walker_queue = 2\n",
    "        while not game.get_game_over() and old_player_making_step == game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "    else:\n",
    "        neuronet_walker_queue = 1\n",
    "\n",
    "    while not game.get_game_over():\n",
    "\n",
    "        # Выбор хода\n",
    "        probs = model(game.get_state().to(torch.double))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # Выбор награды и выполнение хода\n",
    "        reward = 0\n",
    "        old_state = game.get_state().to(torch.double)\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "\n",
    "        rezult_step = game.take_step(action + 1)\n",
    "        # print(\"neuronet took step\", game.get_player_making_step(), \"Был выбор\", action + 1, rezult_step)\n",
    "        if rezult_step == \"Куча пустая, выберите другую!\":\n",
    "            count_choisen_zero += 1\n",
    "            reward = bad_step_reward_stage_2\n",
    "        elif rezult_step == \"Хороший ход!\":\n",
    "            reward = good_step_reward_stage_2\n",
    "            bad_choise = []\n",
    "        elif rezult_step == \"Хороший ход! Захват!\":\n",
    "            reward = good_step_captured_reward_stage_2\n",
    "            bad_choise = []\n",
    "\n",
    "        tr.append([old_state, action, reward, m])\n",
    "\n",
    "        while not game.get_game_over() and old_player_making_step != game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "\n",
    "    rwrd = 0\n",
    "    if game.get_winner() != None:\n",
    "        if game.get_winner() != 0:\n",
    "            if neuronet_walker_queue == game.get_winner():\n",
    "                count_win_neuronet += 1\n",
    "                rwrd += winner_reward_stage_2\n",
    "            else:\n",
    "                rwrd += loser_reward_stage_2\n",
    "        else:\n",
    "            # tr[-1][-2] += draw_reward_stage_2 if tr[-1][-2] > 0 else 0\n",
    "            rwrd += draw_reward_stage_2\n",
    "\n",
    "    loss = 0.\n",
    "    count_played_step = len(tr) # Кол-во сыгранных ходов\n",
    "    for id_current_step in range(count_played_step):\n",
    "        R = 0.\n",
    "        for id_next_step in range(id_current_step, count_played_step):\n",
    "            R += (gamma**(id_current_step-id_next_step))*tr[id_next_step][2] + rwrd\n",
    "        loss += loss_func(model(tr[id_current_step][0]), tr[id_current_step][1], tr[id_current_step][3], R, gamma)\n",
    "\n",
    "    if not check_optim and episode > 10000:\n",
    "        check_optim = True\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 1.0e-5\n",
    "        print(\"updated loss\")\n",
    "\n",
    "    if not episode % 200:\n",
    "        print(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "3253\n"
     ]
    }
   ],
   "source": [
    "print(check_optim)\n",
    "print(count_choisen_zero)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тестирование модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.748\n"
     ]
    }
   ],
   "source": [
    "game = Kalah()\n",
    "episodes_count = 1000\n",
    "neuronet_walker_queue = 1\n",
    "count_win_neuronet = 0\n",
    "count_choisen_zero = 0\n",
    "bad_choise = []\n",
    "rezult_step = \"Хороший ход!\"\n",
    "\n",
    "for episode in range(episodes_count):\n",
    "    game.set_new_game()\n",
    "    if episode % 2:\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "        neuronet_walker_queue = 2\n",
    "        while not game.get_game_over() and old_player_making_step == game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "    else:\n",
    "        neuronet_walker_queue = 1\n",
    "\n",
    "    while not game.get_game_over():\n",
    "\n",
    "        # Выбор хода\n",
    "        probs = model(game.get_state().to(torch.double))\n",
    "        action = probs.argmax()\n",
    "\n",
    "        while action in bad_choise:\n",
    "            probs[probs.argmax()] = 0\n",
    "            action = probs.argmax()\n",
    "\n",
    "        # Выбор награды и выполнение хода\n",
    "        reward = 0\n",
    "        old_state = game.get_state().to(torch.double)\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "\n",
    "        rezult_step = game.take_step(action + 1)\n",
    "        if rezult_step == \"Куча пустая, выберите другую!\":\n",
    "            count_choisen_zero += 1\n",
    "            # reward = bad_step_reward_stage_1\n",
    "            print(\"Neuronet took step\", game.get_player_making_step(), \"Был выбор\", action + 1, rezult_step)\n",
    "            # print(\"state:\")\n",
    "            # game.print_state()\n",
    "            print(\"Номер эпизода происшествия:\", episode)\n",
    "            bad_choise.append(action)\n",
    "        else:\n",
    "            bad_choise = []\n",
    "        # elif rezult_step == \"Хороший ход!\":\n",
    "        #     reward = good_step_reward_stage_1\n",
    "        # elif rezult_step == \"Хороший ход! Захват!\":\n",
    "        #     reward = good_step_captured_reward_stage_1\n",
    "\n",
    "        while not game.get_game_over() and old_player_making_step != game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            if rezult_step == \"Куча пустая, выберите другую!\":\n",
    "                print(\"Bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "\n",
    "\n",
    "    if game.get_winner() != None:\n",
    "        if game.get_winner() != 0:\n",
    "            if neuronet_walker_queue == game.get_winner():\n",
    "                count_win_neuronet += 1\n",
    "                reward += winner_reward_stage_2\n",
    "            else:\n",
    "                reward += loser_reward_stage_2\n",
    "        else:\n",
    "            reward += draw_reward_stage_2\n",
    "\n",
    "    # print(\"Очередь хода нейронки\", neuronet_walker_queue)\n",
    "    # print(\"Результат:\", game.get_player_winner(), 'k', game.get_winner())\n",
    "    # print(\"Номер попытки:\", episode)\n",
    "    # print(\"Награда/Штраф:\", reward)\n",
    "\n",
    "    # print(episode)\n",
    "\n",
    "print(count_choisen_zero)\n",
    "print(count_win_neuronet/episodes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_save = True\n",
    "# is_save = False\n",
    "if is_save:\n",
    "    torch.save(model, \"./model_winner_ver4.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d71e8b1932faf56dee17752329eeb73c746bdacdb1acbdc067bcd5bd3a88241"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
