{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Первый этап, обучение на правильность ходов***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выполнил Кузин Мирослав**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Траектория:\n",
    "$$\\tau = (s_0,a_0,r_0),\\dots,(s_T,a_T,r_T) ,$$\n",
    "где $s_i$ состояние среды на шаге $i$, $a_i$ воздействие агента на среду на шаге $i$,\n",
    "$r_i$ вознаграждение на шаге $i$.\n",
    "\n",
    "Вознаграждение начиная с шага $t$:\n",
    "$$R_t(\\tau)=\\sum\\limits_{j=t}^{T}\\gamma^{j-t}r_{j}, $$\n",
    "$$R(\\tau) = R_0(\\tau) $$\n",
    "где $\\gamma$ дисконтный множитель (коэффициент значимости вознаграждения при переходе на следущий шаг)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение с подкреплением - набор методов, позволяющих агенту (интеллектуальной системе) вырабатывать оптимальную стратегию при его взаимодействии со средой (внешним миром). \n",
    "\n",
    "Среда предоставляет информацию, описывающую состояние системы. Агент взаимодействует со средой, наблюдая состояние и используя данную информацию при выборе действия. Среда принимает действие и переходит в следующее состояние, а затем возвращает агенту следующее состояние и вознаграждение. Когда цикл «состояние → действие → вознаграждение» завершен, предполагается, что сделан один шаг. Цикл повторяется, пока среда не завершится, например, когда задача решена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия $\\pi$ — это функция, отображающая состояния вероятности действий, которые используются для выбора действия $a \\sim \\pi(s) $. \n",
    "\n",
    "Стратегия $\\pi$ содержит настраеваемые параметры $\\omega$, чтобы это подчеркнуть будем писать $\\pi_{\\omega}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целевая функция — это ожидаемая отдача по всем полным траекториям, порожденным агентом.\n",
    "$$J(\\omega)=J(\\pi_{\\omega})=M_{\\tau\\sim \\omega}R(\\tau),$$\n",
    "где $M_{\\tau\\sim \\omega}$ математическое ожидание по всех траекториям $\\tau$ соответствующим значениям параметров $\\omega$.\n",
    "\n",
    "Задача:\n",
    "$$J(\\omega)\\to \\max $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\omega}J=\\nabla_{\\omega} M_{\\tau\\sim \\omega}R(\\tau)$$\n",
    "\n",
    "$$\\nabla_{\\omega}M_{\\tau\\sim \\omega}R(\\tau)=\\nabla_{\\omega}\\int R(\\tau)p(\\tau\\mid\\omega)d\\tau= $$\n",
    "\n",
    "$$=\\int \\nabla_{\\omega}(R(\\tau)p(\\tau\\mid\\omega))d\\tau =\\int (\\nabla_{\\omega}(R(\\tau))p(\\tau\\mid\\omega)+R(\\tau)\\nabla_{\\omega}(p(\\tau\\mid\\omega)))d\\tau =$$\n",
    "\n",
    "$$ =\\int R(\\tau)\\nabla_{\\omega}(p(\\tau\\mid\\omega))d\\tau =\\int R(\\tau)\\nabla_{\\omega}(p(\\tau\\mid\\omega)) \\dfrac{p(\\tau\\mid\\omega)}{p(\\tau\\mid\\omega)}d\\tau =$$\n",
    "\n",
    "$$ =\\int R(\\tau)\\dfrac{\\nabla_{\\omega}(p(\\tau\\mid\\omega)) }{p(\\tau\\mid\\omega)}p(\\tau\\mid\\omega)d\\tau =\\int R(\\tau)\\nabla_{\\omega}(\\ln p(\\tau\\mid\\omega))p(\\tau\\mid\\omega)d\\tau =$$\n",
    "\n",
    "$$ = M_{\\tau\\sim \\omega}(R(\\tau)\\nabla_{\\omega}(\\ln p(\\tau\\mid\\omega)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\tau\\mid\\omega)=\\prod\\limits_t p(s_{s+1}\\mid s_t,a_t)\\pi_{\\omega}(a_t\\mid s_t) $$\n",
    "\n",
    "$$\\ln p(\\tau\\mid\\omega)=\\ln\\prod\\limits_t p(s_{s+1}\\mid s_t,a_t)\\pi_{\\omega}(a_t\\mid s_t) = \\sum\\limits_t (\\ln p(s_{s+1}\\mid s_t,a_t)+\\ln\\pi_{\\omega}(a_t\\mid s_t))$$\n",
    "\n",
    "$$\\nabla_{\\omega}\\ln p(\\tau\\mid\\omega)=\\nabla_{\\omega}\\sum\\limits_t (\\ln p(s_{s+1}\\mid s_t,a_t)+\\ln\\pi_{\\omega}(a_t\\mid s_t))=\\nabla_{\\omega}\\sum\\limits_t \\ln\\pi_{\\omega}(a_t\\mid s_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss(\\omega) = - R(\\tau)\\sum\\limits_t \\ln\\pi_{\\omega}(a_t\\mid s_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mancala import Kalah\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Болваны**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Бот простой, рандомно выбирает не нулевые элементы\n",
    "def do_simple_bot_step(state: torch.Tensor) -> int:\n",
    "    nonzero_state_indexs = torch.nonzero(state[:6]).flatten()\n",
    "    rez = nonzero_state_indexs[torch.randint(0, len(nonzero_state_indexs), (1,))[0]]\n",
    "    return rez + 1\n",
    "\n",
    "do_simple_bot_step(torch.Tensor([1, 0, 1, 0, 0, 0, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(14, 42),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(42, 6),\n",
    "    torch.nn.Softmax(dim = -1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Настройка обучения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def loss_func(probs: torch.Tensor, action: int, m: Categorical, R: int, gamma: int):\n",
    "    alpha = 0.9\n",
    "    # print(Categorical(probs).log_prob(action))\n",
    "    return -alpha*R*m.log_prob(action)\n",
    "\n",
    "loss = loss_func\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3, amsgrad=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.3706)\n"
     ]
    }
   ],
   "source": [
    "m = Categorical(torch.Tensor([0.9, 0.1]))\n",
    "action = m.sample()\n",
    "print(loss_func(torch.Tensor([0.9, 0.1]), action, m, -25, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 6, 6, 6, 6, 0, 0, 6, 6, 6, 6, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0476, 0.0784, 0.6468, 0.1430, 0.0341, 0.0500], dtype=torch.float64,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка работы модели\n",
    "game = Kalah()\n",
    "model.to(device)\n",
    "model.to(torch.double)\n",
    "\n",
    "print(game.get_general_state())\n",
    "model(game.get_general_state().to(torch.double))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Награды и штрафы*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первый этап обучения\n",
    "winner_reward_stage_1 = 1e-3\n",
    "loser_reward_stage_1 = -1e-3\n",
    "draw_reward_stage_1 = -0.5e-3\n",
    "bad_step_reward_stage_1 = -25 # общий\n",
    "good_step_reward_stage_1 = 2\n",
    "good_step_captured_reward_stage_1 = 4\n",
    "# bad_step_reward_stage_1 = -25 # общий\n",
    "# good_step_reward_stage_1 = 1\n",
    "# good_step_captured_reward_stage_1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.7437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "updated loss\n",
      "tensor(25.9859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(-72.7508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(63.2375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48.8271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(33.3220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(86.2056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(49.0961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(27.1993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(57.7266, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "game = Kalah()\n",
    "episodes_count = 1000\n",
    "neuronet_walker_queue = 1\n",
    "count_win_neuronet = 0\n",
    "count_choisen_zero = 0\n",
    "gamma = 2\n",
    "loss_story = []\n",
    "check_optim = False\n",
    "\n",
    "for episode in range(0, episodes_count):\n",
    "    game.set_new_game()\n",
    "    tr = []\n",
    "    if episode % 2:\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "        neuronet_walker_queue = 2\n",
    "        while not game.get_game_over() and old_player_making_step == game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "    else:\n",
    "        neuronet_walker_queue = 1\n",
    "\n",
    "    while not game.get_game_over():\n",
    "\n",
    "        # Выбор хода\n",
    "        probs = model(game.get_state().to(torch.double))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # Выбор нагады и выполнение хода\n",
    "        reward = 0\n",
    "        old_state = game.get_state().to(torch.double)\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "\n",
    "        rezult_step = game.take_step(action + 1)\n",
    "        # print(\"neuronet took step\", game.get_player_making_step(), \"Был выбор\", action + 1, rezult_step)\n",
    "        if rezult_step == \"Куча пустая, выберите другую!\":\n",
    "            count_choisen_zero += 1\n",
    "            reward = bad_step_reward_stage_1\n",
    "        elif rezult_step == \"Хороший ход!\":\n",
    "            reward = good_step_reward_stage_1\n",
    "        elif rezult_step == \"Хороший ход! Захват!\":\n",
    "            reward = good_step_captured_reward_stage_1\n",
    "\n",
    "        tr.append((old_state, action, reward, m))\n",
    "\n",
    "        while not game.get_game_over() and old_player_making_step != game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "\n",
    "    if game.get_winner() != None:\n",
    "        if game.get_winner() != 0:\n",
    "            if neuronet_walker_queue == game.get_winner():\n",
    "                count_win_neuronet += 1\n",
    "                reward += winner_reward_stage_1\n",
    "            else:\n",
    "                reward += loser_reward_stage_1\n",
    "        else:\n",
    "            reward += draw_reward_stage_1\n",
    "\n",
    "    loss = 0.\n",
    "    count_played_step = len(tr) # Кол-во сыгранных ходов\n",
    "    for id_current_step in range(count_played_step):\n",
    "        R = 0.\n",
    "        for id_next_step in range(id_current_step, count_played_step):\n",
    "            R += (gamma**(id_current_step-id_next_step))*tr[id_next_step][2]\n",
    "        # R += rwd_in_end_game\n",
    "        loss += loss_func(model(tr[id_current_step][0]), tr[id_current_step][1], tr[id_current_step][3], R, gamma)\n",
    "\n",
    "    # loss_story.append(loss)\n",
    "\n",
    "    if not check_optim and episode > 1:\n",
    "        check_optim = True\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 1.0e-5\n",
    "        print(\"updated loss\")\n",
    "\n",
    "    if not episode % 100:\n",
    "        print(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "print(check_optim)\n",
    "print(count_choisen_zero)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тестирование модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.703\n"
     ]
    }
   ],
   "source": [
    "game = Kalah()\n",
    "episodes_count = 1000\n",
    "neuronet_walker_queue = 1\n",
    "count_win_neuronet = 0\n",
    "count_choisen_zero = 0\n",
    "bad_choise = []\n",
    "rezult_step = \"Хороший ход!\"\n",
    "\n",
    "for episode in range(episodes_count):\n",
    "    game.set_new_game()\n",
    "    if episode % 2:\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "        neuronet_walker_queue = 2\n",
    "        while not game.get_game_over() and old_player_making_step == game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "    else:\n",
    "        neuronet_walker_queue = 1\n",
    "\n",
    "    while not game.get_game_over():\n",
    "\n",
    "        # Выбор хода\n",
    "        probs = model(game.get_state().to(torch.double))\n",
    "        action = probs.argmax()\n",
    "\n",
    "        while action in bad_choise:\n",
    "            probs[probs.argmax()] = 0\n",
    "            action = probs.argmax()\n",
    "\n",
    "        # Выбор награды и выполнение хода\n",
    "        reward = 0\n",
    "        old_state = game.get_state().to(torch.double)\n",
    "        old_player_making_step = game.get_player_making_step()\n",
    "\n",
    "        rezult_step = game.take_step(action + 1)\n",
    "        if rezult_step == \"Куча пустая, выберите другую!\":\n",
    "            count_choisen_zero += 1\n",
    "            # reward = bad_step_reward_stage_1\n",
    "            print(\"Neuronet took step\", game.get_player_making_step(), \"Был выбор\", action + 1, rezult_step)\n",
    "            # print(\"state:\")\n",
    "            # game.print_state()\n",
    "            print(\"Номер эпизода происшествия:\", episode)\n",
    "            bad_choise.append(action)\n",
    "        else:\n",
    "            bad_choise = []\n",
    "        # elif rezult_step == \"Хороший ход!\":\n",
    "        #     reward = good_step_reward_stage_1\n",
    "        # elif rezult_step == \"Хороший ход! Захват!\":\n",
    "        #     reward = good_step_captured_reward_stage_1\n",
    "\n",
    "        while not game.get_game_over() and old_player_making_step != game.get_player_making_step():\n",
    "            bot_action = do_simple_bot_step(game.get_state())\n",
    "            rezult_step = game.take_step(bot_action)\n",
    "            if rezult_step == \"Куча пустая, выберите другую!\":\n",
    "                print(\"Bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "            # print(\"bot took step\", game.get_player_making_step(), \"Был выбор\", bot_action, rezult_step)\n",
    "\n",
    "\n",
    "    if game.get_winner() != None:\n",
    "        if game.get_winner() != 0:\n",
    "            if neuronet_walker_queue == game.get_winner():\n",
    "                count_win_neuronet += 1\n",
    "                reward += winner_reward_stage_1\n",
    "            else:\n",
    "                reward += loser_reward_stage_1\n",
    "        else:\n",
    "            reward += draw_reward_stage_1\n",
    "\n",
    "    # print(\"Очередь хода нейронки\", neuronet_walker_queue)\n",
    "    # print(\"Результат:\", game.get_player_winner(), 'k', game.get_winner())\n",
    "    # print(\"Номер попытки:\", episode)\n",
    "    # print(\"Награда/Штраф:\", reward)\n",
    "\n",
    "    # print(episode)\n",
    "\n",
    "print(count_choisen_zero)\n",
    "print(count_win_neuronet/episodes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print(count_choisen_zero)\n",
    "print(count_win_neuronet/episodes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1) tensor(0)\n",
      "tensor(-2.3026)\n"
     ]
    }
   ],
   "source": [
    "probs = torch.Tensor([0.9, 0.1])\n",
    "m = Categorical(probs)\n",
    "action = m.sample()\n",
    "print(action)\n",
    "# t = Categorical(probs)\n",
    "# print(action, t.sample())\n",
    "print(m.log_prob(action))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_save = True\n",
    "is_save = False\n",
    "if is_save:\n",
    "    torch.save(model, \"./model_non_zero_top_tryed3.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d71e8b1932faf56dee17752329eeb73c746bdacdb1acbdc067bcd5bd3a88241"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
