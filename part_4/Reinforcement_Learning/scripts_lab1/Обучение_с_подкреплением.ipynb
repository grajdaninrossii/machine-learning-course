{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx7cf-3XJ1na"
      },
      "source": [
        "# Обучение с подкреплением"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfVsNApLPs-"
      },
      "source": [
        "https://qudata.com/ml/ru/RL_Reinforcement_learning.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbnqfNTKMcFR"
      },
      "source": [
        "Обучение с подкреплением (Reinforcement Learning, RL) - набор методов, позволяющих агенту (интеллектуальной системе) вырабатывать оптимальную стратегию при его взаимодействии со средой (внешним миром). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFoFgRlMhod"
      },
      "source": [
        "# Траектория:\n",
        "$$\\tau = (s_0,a_0,r_0),\\dots,(s_T,a_T,r_T) ,$$\n",
        "где $s_i$ состояние среды на шаге $i$, $a_i$ воздействие агента на среду на шаге $i$,\n",
        "$r_i$ вознаграждение на шаге $i$.\n",
        "\n",
        "Вознаграждение начиная с шага $t$:\n",
        "$$R_t(\\tau)=\\sum\\limits_{j=t}^{T}\\gamma^{j-t}r_{j}, $$\n",
        "$$R(\\tau) = R_0(\\tau) $$\n",
        "где $\\gamma$ дисконтный множитель (коэффициент значимости вознаграждения при переходе на следущий шаг)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QLE_eC0OKt6"
      },
      "source": [
        "Обучение с подкреплением - набор методов, позволяющих агенту (интеллектуальной системе) вырабатывать оптимальную стратегию при его взаимодействии со средой (внешним миром). \n",
        "\n",
        "Среда предоставляет информацию, описывающую состояние системы. Агент взаимодействует со средой, наблюдая состояние и используя данную информацию при выборе действия. Среда принимает действие и переходит в следующее состояние, а затем возвращает агенту следующее состояние и вознаграждение. Когда цикл «состояние → действие → вознаграждение» завершен, предполагается, что сделан один шаг. Цикл повторяется, пока среда не завершится, например, когда задача решена."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x7WKIMMOaFM"
      },
      "source": [
        "Стратегия $\\pi$ — это функция, отображающая состояния вероятности действий, которые используются для выбора действия $a \\sim \\pi(s) $. \n",
        "\n",
        "Стратегия $\\pi$ содержит настраеваемые параметры $\\omega$, чтобы это подчеркнуть будем писать $\\pi_{\\omega}(s)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dCDuthFPEnF"
      },
      "source": [
        "Целевая функция — это ожидаемая отдача по всем полным траекториям, порожденным агентом.\n",
        "$$J(\\omega)=J(\\pi_{\\omega})=M_{\\tau\\sim \\omega}R(\\tau),$$\n",
        "где $M_{\\tau\\sim \\omega}$ математическое ожидание по всех траекториям $\\tau$ соответствующим значениям параметров $\\omega$.\n",
        "\n",
        "Задача:\n",
        "$$J(\\omega)\\to \\max $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5siVdTJPrBH"
      },
      "source": [
        "$$\\nabla_{\\omega}J=\\nabla_{\\omega} M_{\\tau\\sim \\omega}R(\\tau)$$\n",
        "\n",
        "$$\\nabla_{\\omega}M_{\\tau\\sim \\omega}R(\\tau)=\\nabla_{\\omega}\\int R(\\tau)p(\\tau\\mid\\omega)d\\tau= $$\n",
        "\n",
        "$$=\\int \\nabla_{\\omega}(R(\\tau)p(\\tau\\mid\\omega))d\\tau =\\int (\\nabla_{\\omega}(R(\\tau))p(\\tau\\mid\\omega)+R(\\tau)\\nabla_{\\omega}(p(\\tau\\mid\\omega)))d\\tau =$$\n",
        "\n",
        "$$ =\\int R(\\tau)\\nabla_{\\omega}(p(\\tau\\mid\\omega))d\\tau =\\int R(\\tau)\\nabla_{\\omega}(p(\\tau\\mid\\omega)) \\dfrac{p(\\tau\\mid\\omega)}{p(\\tau\\mid\\omega)}d\\tau =$$\n",
        "\n",
        "$$ =\\int R(\\tau)\\dfrac{\\nabla_{\\omega}(p(\\tau\\mid\\omega)) }{p(\\tau\\mid\\omega)}p(\\tau\\mid\\omega)d\\tau =\\int R(\\tau)\\nabla_{\\omega}(\\ln p(\\tau\\mid\\omega))p(\\tau\\mid\\omega)d\\tau =$$\n",
        "\n",
        "$$ = M_{\\tau\\sim \\omega}(R(\\tau)\\nabla_{\\omega}(\\ln p(\\tau\\mid\\omega)))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riW7GVq_RUN-"
      },
      "source": [
        "$$p(\\tau\\mid\\omega)=\\prod\\limits_t p(s_{s+1}\\mid s_t,a_t)\\pi_{\\omega}(a_t\\mid s_t) $$\n",
        "\n",
        "$$\\ln p(\\tau\\mid\\omega)=\\ln\\prod\\limits_t p(s_{s+1}\\mid s_t,a_t)\\pi_{\\omega}(a_t\\mid s_t) = \\sum\\limits_t (\\ln p(s_{s+1}\\mid s_t,a_t)+\\ln\\pi_{\\omega}(a_t\\mid s_t))$$\n",
        "\n",
        "$$\\nabla_{\\omega}\\ln p(\\tau\\mid\\omega)=\\nabla_{\\omega}\\sum\\limits_t (\\ln p(s_{s+1}\\mid s_t,a_t)+\\ln\\pi_{\\omega}(a_t\\mid s_t))=\\nabla_{\\omega}\\sum\\limits_t \\ln\\pi_{\\omega}(a_t\\mid s_t)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMnJYC9OSjfv"
      },
      "source": [
        "$$Loss(\\omega) = - R(\\tau)\\sum\\limits_t \\ln\\pi_{\\omega}(a_t\\mid s_t) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wCw3joc4Mv4n"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vXBMcT7FTwNY"
      },
      "outputs": [],
      "source": [
        "DL = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pHXbu0DEUBiz"
      },
      "outputs": [],
      "source": [
        "def space():\n",
        "    return torch.tensor(np.random.permutation(DL)) + 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09CBFc5rUIV2",
        "outputId": "2c9296f1-0a1b-446b-934e-1da539c784e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([6., 1., 7., 5., 2., 4., 9., 8., 3.])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "space()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "q0IRJ6eRULjt"
      },
      "outputs": [],
      "source": [
        "Prs = 10\n",
        "model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(DL,Prs),\n",
        "        torch.nn.Sigmoid(),\n",
        "        torch.nn.Linear(Prs, DL),\n",
        "        torch.nn.Softmax()\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj7nichEWVvJ",
        "outputId": "ca698234-a312-4b7b-ca7b-ee126756f09e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0719, 0.1002, 0.0852, 0.1098, 0.0775, 0.1424, 0.1476, 0.1495, 0.1159],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = space()\n",
        "\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7LmuAjtWpdy",
        "outputId": "83d47c19-af1e-4d39-dab5-d2d24b992422"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5., 3., 7., 4., 8., 6., 2., 9., 1.])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Nrt-isMIWwgc"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "guJE0npLXb17"
      },
      "outputs": [],
      "source": [
        "def bolvan(x):\n",
        "    x[np.random.randint(DL)] = -1.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfFMqJbVW4eS",
        "outputId": "f7ecb1b3-ef8f-4a3c-8a1d-cc8cd5dbff0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kuzin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "8\n",
            "20\n",
            "8\n",
            "7\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "6\n",
            "6\n",
            "9\n",
            "10\n",
            "7\n",
            "9\n",
            "7\n",
            "7\n",
            "10\n",
            "8\n",
            "6\n",
            "11\n",
            "6\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "6\n",
            "8\n",
            "9\n",
            "14\n",
            "10\n",
            "5\n",
            "6\n",
            "7\n",
            "10\n",
            "10\n",
            "6\n",
            "9\n",
            "8\n",
            "5\n",
            "14\n",
            "13\n",
            "6\n",
            "7\n",
            "7\n",
            "5\n",
            "7\n",
            "8\n",
            "6\n",
            "6\n",
            "9\n",
            "7\n",
            "7\n",
            "7\n",
            "6\n",
            "14\n",
            "7\n",
            "16\n",
            "7\n",
            "7\n",
            "7\n",
            "8\n",
            "8\n",
            "6\n",
            "10\n",
            "6\n",
            "7\n",
            "12\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "13\n",
            "7\n",
            "5\n",
            "7\n",
            "7\n",
            "12\n",
            "10\n",
            "10\n",
            "7\n",
            "8\n",
            "6\n",
            "8\n",
            "6\n",
            "8\n",
            "10\n",
            "6\n",
            "7\n",
            "9\n",
            "7\n",
            "7\n",
            "8\n",
            "7\n",
            "5\n",
            "6\n",
            "6\n",
            "7\n",
            "10\n",
            "7\n",
            "7\n",
            "8\n",
            "9\n",
            "7\n",
            "9\n",
            "7\n",
            "5\n",
            "7\n",
            "11\n",
            "5\n",
            "5\n",
            "13\n",
            "7\n",
            "8\n",
            "10\n",
            "9\n",
            "6\n",
            "9\n",
            "6\n",
            "10\n",
            "7\n",
            "14\n",
            "5\n",
            "9\n",
            "6\n",
            "8\n",
            "5\n",
            "7\n",
            "10\n",
            "9\n",
            "6\n",
            "7\n",
            "6\n",
            "8\n",
            "7\n",
            "6\n",
            "10\n",
            "6\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "6\n",
            "9\n",
            "5\n",
            "9\n",
            "10\n",
            "9\n",
            "5\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "6\n",
            "7\n",
            "8\n",
            "6\n",
            "6\n",
            "7\n",
            "13\n",
            "16\n",
            "7\n",
            "8\n",
            "8\n",
            "5\n",
            "10\n",
            "8\n",
            "6\n",
            "6\n",
            "8\n",
            "12\n",
            "7\n",
            "7\n",
            "9\n",
            "11\n",
            "6\n",
            "7\n",
            "9\n",
            "9\n",
            "9\n",
            "8\n",
            "6\n",
            "9\n",
            "8\n",
            "11\n",
            "7\n",
            "7\n",
            "8\n",
            "5\n",
            "10\n",
            "5\n",
            "8\n",
            "9\n",
            "10\n",
            "6\n",
            "11\n",
            "8\n",
            "8\n",
            "6\n",
            "6\n",
            "8\n",
            "13\n",
            "6\n",
            "9\n",
            "7\n",
            "7\n",
            "9\n",
            "6\n",
            "8\n",
            "9\n",
            "6\n",
            "8\n",
            "11\n",
            "8\n",
            "8\n",
            "8\n",
            "5\n",
            "8\n",
            "8\n",
            "9\n",
            "7\n",
            "10\n",
            "12\n",
            "9\n",
            "7\n",
            "8\n",
            "7\n",
            "7\n",
            "6\n",
            "7\n",
            "12\n",
            "9\n",
            "8\n",
            "14\n",
            "8\n",
            "10\n",
            "11\n",
            "9\n",
            "6\n",
            "11\n",
            "8\n",
            "7\n",
            "7\n",
            "11\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "7\n",
            "6\n",
            "6\n",
            "8\n",
            "10\n",
            "6\n",
            "8\n",
            "8\n",
            "8\n",
            "12\n",
            "5\n",
            "9\n",
            "7\n",
            "8\n",
            "7\n",
            "6\n",
            "8\n",
            "5\n",
            "7\n",
            "13\n",
            "7\n",
            "10\n",
            "9\n",
            "8\n",
            "11\n",
            "6\n",
            "6\n",
            "5\n",
            "6\n",
            "6\n",
            "5\n",
            "8\n",
            "6\n",
            "12\n",
            "8\n",
            "11\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "5\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "11\n",
            "10\n",
            "13\n",
            "6\n",
            "16\n",
            "6\n",
            "7\n",
            "9\n",
            "13\n",
            "6\n",
            "7\n",
            "9\n",
            "9\n",
            "6\n",
            "7\n",
            "7\n",
            "11\n",
            "6\n",
            "5\n",
            "10\n",
            "9\n",
            "7\n",
            "13\n",
            "10\n",
            "8\n",
            "6\n",
            "9\n",
            "6\n",
            "7\n",
            "8\n",
            "7\n",
            "8\n",
            "10\n",
            "8\n",
            "6\n",
            "8\n",
            "6\n",
            "9\n",
            "10\n",
            "7\n",
            "9\n",
            "11\n",
            "9\n",
            "9\n",
            "8\n",
            "12\n",
            "11\n",
            "6\n",
            "10\n",
            "5\n",
            "9\n",
            "7\n",
            "7\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "7\n",
            "8\n",
            "5\n",
            "12\n",
            "7\n",
            "6\n",
            "7\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "6\n",
            "7\n",
            "10\n",
            "5\n",
            "9\n",
            "7\n",
            "9\n",
            "8\n",
            "6\n",
            "11\n",
            "9\n",
            "7\n",
            "13\n",
            "8\n",
            "9\n",
            "6\n",
            "9\n",
            "12\n",
            "13\n",
            "14\n",
            "8\n",
            "7\n",
            "12\n",
            "11\n",
            "7\n",
            "5\n",
            "5\n",
            "5\n",
            "5\n",
            "10\n",
            "11\n",
            "4\n",
            "8\n",
            "5\n",
            "14\n",
            "6\n",
            "9\n",
            "11\n",
            "6\n",
            "10\n",
            "12\n",
            "8\n",
            "6\n",
            "6\n",
            "8\n",
            "5\n",
            "7\n",
            "6\n",
            "9\n",
            "6\n",
            "8\n",
            "7\n",
            "6\n",
            "6\n",
            "7\n",
            "7\n",
            "6\n",
            "7\n",
            "6\n",
            "12\n",
            "6\n",
            "6\n",
            "7\n",
            "11\n",
            "7\n",
            "5\n",
            "6\n",
            "8\n",
            "6\n",
            "6\n",
            "7\n",
            "7\n",
            "8\n",
            "11\n",
            "9\n",
            "6\n",
            "6\n",
            "10\n",
            "9\n",
            "7\n",
            "11\n",
            "10\n",
            "8\n",
            "13\n",
            "8\n",
            "15\n",
            "6\n",
            "11\n",
            "6\n",
            "5\n",
            "7\n",
            "9\n",
            "6\n",
            "7\n",
            "7\n",
            "9\n",
            "7\n",
            "9\n",
            "10\n",
            "13\n",
            "8\n",
            "7\n",
            "8\n",
            "9\n",
            "7\n",
            "7\n",
            "8\n",
            "7\n",
            "10\n",
            "7\n",
            "6\n",
            "6\n",
            "8\n",
            "8\n",
            "6\n",
            "8\n",
            "8\n",
            "5\n",
            "7\n",
            "5\n",
            "9\n",
            "11\n",
            "12\n",
            "8\n",
            "7\n",
            "7\n",
            "9\n",
            "8\n",
            "10\n",
            "6\n",
            "9\n",
            "8\n",
            "10\n",
            "9\n",
            "9\n",
            "7\n",
            "19\n",
            "8\n",
            "13\n",
            "10\n",
            "8\n",
            "9\n",
            "7\n",
            "6\n",
            "5\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "8\n",
            "14\n",
            "7\n",
            "6\n",
            "9\n",
            "6\n",
            "9\n",
            "10\n",
            "6\n",
            "6\n",
            "7\n",
            "5\n",
            "7\n",
            "6\n",
            "7\n",
            "8\n",
            "7\n",
            "6\n",
            "6\n",
            "7\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "8\n",
            "7\n",
            "8\n",
            "6\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "10\n",
            "8\n",
            "7\n",
            "11\n",
            "8\n",
            "7\n",
            "8\n",
            "7\n",
            "6\n",
            "6\n",
            "13\n",
            "9\n",
            "8\n",
            "8\n",
            "7\n",
            "5\n",
            "8\n",
            "9\n",
            "6\n",
            "7\n",
            "7\n",
            "7\n",
            "7\n",
            "7\n",
            "5\n",
            "10\n",
            "11\n",
            "13\n",
            "6\n",
            "7\n",
            "10\n",
            "10\n",
            "8\n",
            "6\n",
            "14\n",
            "6\n",
            "10\n",
            "5\n",
            "7\n",
            "9\n",
            "7\n",
            "8\n",
            "8\n",
            "7\n",
            "7\n",
            "8\n",
            "7\n",
            "4\n",
            "7\n",
            "7\n",
            "7\n",
            "8\n",
            "8\n",
            "5\n",
            "11\n",
            "9\n",
            "6\n",
            "6\n",
            "9\n",
            "6\n",
            "12\n",
            "7\n",
            "7\n",
            "13\n",
            "8\n",
            "7\n",
            "8\n",
            "13\n",
            "8\n",
            "6\n",
            "7\n",
            "6\n",
            "6\n",
            "6\n",
            "7\n",
            "11\n",
            "7\n",
            "5\n",
            "8\n",
            "9\n",
            "10\n",
            "8\n",
            "9\n",
            "7\n",
            "7\n",
            "6\n",
            "8\n",
            "8\n",
            "9\n",
            "6\n",
            "7\n",
            "10\n",
            "10\n",
            "6\n",
            "9\n",
            "8\n",
            "7\n",
            "8\n",
            "12\n",
            "8\n",
            "6\n",
            "8\n",
            "11\n",
            "10\n",
            "8\n",
            "7\n",
            "8\n",
            "5\n",
            "7\n",
            "12\n",
            "11\n",
            "6\n",
            "8\n",
            "7\n",
            "10\n",
            "11\n",
            "7\n",
            "6\n",
            "5\n",
            "9\n",
            "8\n",
            "6\n",
            "7\n",
            "7\n",
            "8\n",
            "6\n",
            "8\n",
            "10\n",
            "7\n",
            "7\n",
            "12\n",
            "8\n",
            "10\n",
            "7\n",
            "9\n",
            "12\n",
            "7\n",
            "11\n",
            "12\n",
            "8\n",
            "7\n",
            "7\n",
            "7\n",
            "8\n",
            "8\n",
            "8\n",
            "7\n",
            "8\n",
            "8\n",
            "9\n",
            "6\n",
            "9\n",
            "6\n",
            "6\n",
            "8\n",
            "10\n",
            "7\n",
            "6\n",
            "6\n",
            "8\n",
            "7\n",
            "8\n",
            "8\n",
            "8\n",
            "7\n",
            "7\n",
            "9\n",
            "8\n",
            "10\n",
            "6\n",
            "6\n",
            "9\n",
            "7\n",
            "6\n",
            "6\n",
            "7\n",
            "8\n",
            "11\n",
            "7\n",
            "10\n",
            "8\n",
            "7\n",
            "8\n",
            "7\n",
            "6\n",
            "12\n",
            "10\n",
            "7\n",
            "8\n",
            "6\n",
            "6\n",
            "5\n",
            "14\n",
            "6\n",
            "6\n",
            "7\n",
            "6\n",
            "6\n",
            "7\n",
            "10\n",
            "10\n",
            "7\n",
            "7\n",
            "5\n",
            "7\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "7\n",
            "6\n",
            "6\n",
            "9\n",
            "9\n",
            "6\n",
            "7\n",
            "5\n",
            "6\n",
            "8\n",
            "8\n",
            "5\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "8\n",
            "5\n",
            "10\n",
            "7\n",
            "9\n",
            "11\n",
            "6\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "6\n",
            "8\n",
            "6\n",
            "6\n",
            "7\n",
            "10\n",
            "6\n",
            "8\n",
            "7\n",
            "10\n",
            "9\n",
            "6\n",
            "6\n",
            "15\n",
            "13\n",
            "7\n",
            "7\n",
            "5\n",
            "7\n",
            "7\n",
            "6\n",
            "7\n",
            "7\n",
            "9\n",
            "8\n",
            "8\n",
            "6\n",
            "9\n",
            "13\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "7\n",
            "7\n",
            "6\n",
            "8\n",
            "7\n",
            "10\n",
            "12\n",
            "7\n",
            "7\n",
            "7\n",
            "11\n",
            "6\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "8\n",
            "10\n",
            "6\n",
            "9\n",
            "6\n",
            "7\n",
            "6\n",
            "10\n",
            "12\n",
            "13\n",
            "11\n",
            "7\n",
            "7\n",
            "6\n",
            "6\n",
            "8\n",
            "6\n",
            "5\n",
            "7\n",
            "6\n",
            "8\n",
            "8\n",
            "6\n",
            "8\n",
            "8\n",
            "6\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "16\n",
            "8\n",
            "7\n",
            "8\n",
            "5\n",
            "11\n",
            "9\n",
            "10\n",
            "11\n",
            "7\n",
            "8\n",
            "7\n",
            "10\n",
            "7\n",
            "6\n",
            "7\n",
            "7\n",
            "8\n",
            "13\n",
            "8\n",
            "8\n",
            "7\n",
            "7\n",
            "6\n",
            "6\n",
            "19\n",
            "6\n",
            "8\n",
            "10\n",
            "7\n",
            "6\n",
            "7\n",
            "6\n",
            "5\n",
            "7\n",
            "7\n",
            "10\n",
            "7\n",
            "8\n",
            "8\n",
            "6\n",
            "10\n",
            "9\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "5\n",
            "9\n",
            "6\n",
            "7\n",
            "7\n",
            "10\n",
            "6\n",
            "8\n",
            "11\n",
            "8\n",
            "10\n",
            "6\n",
            "8\n",
            "7\n",
            "10\n",
            "7\n",
            "10\n",
            "6\n",
            "7\n",
            "15\n",
            "8\n",
            "10\n",
            "17\n",
            "13\n",
            "6\n",
            "6\n",
            "6\n",
            "7\n",
            "6\n",
            "11\n",
            "5\n",
            "8\n",
            "7\n",
            "6\n",
            "11\n",
            "7\n",
            "9\n",
            "8\n",
            "7\n",
            "8\n",
            "9\n",
            "7\n",
            "8\n",
            "8\n",
            "7\n",
            "6\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "7\n",
            "7\n",
            "13\n",
            "8\n",
            "10\n",
            "7\n",
            "6\n",
            "7\n",
            "7\n",
            "5\n",
            "8\n",
            "8\n",
            "7\n",
            "5\n",
            "6\n",
            "9\n",
            "11\n",
            "6\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     37\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 38\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     41\u001b[0m \u001b[39mprint\u001b[39m(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m time_1)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m--> 393\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "time_1 = time.time()\n",
        "\n",
        "gamma = 1\n",
        "alpha = 0.9\n",
        "max_ep = 1000\n",
        "\n",
        "for episode in range(max_ep):\n",
        "    SPACE = space()\n",
        "    tr = []\n",
        "    if episode%2:\n",
        "        SPACE = bolvan(SPACE)\n",
        "\n",
        "    while SPACE.sum() + DL:\n",
        "\n",
        "        probs = model(SPACE)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        reward = SPACE[action]\n",
        "        tr.append((SPACE.clone(), action, reward))\n",
        "        SPACE[action] = -1.\n",
        "\n",
        "        SPACE = bolvan(SPACE)\n",
        "\n",
        "    loss = 0.\n",
        "    T = len(tr)\n",
        "    for t in range(T):\n",
        "        R = 0.\n",
        "        for i in range(t,T):\n",
        "            R += (gamma**(i - t))*tr[i][2]\n",
        "\n",
        "        loss += -alpha*R*Categorical(model(tr[t][0])).log_prob(tr[t][1])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "print(time.time() - time_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiScBKVCaICM",
        "outputId": "16979238-45ec-496d-9d7d-e4ad63246168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([7., 5., 1., 3., 2., 9., 8., 6., 4.])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([0.0585, 0.0893, 0.0693, 0.1153, 0.1045, 0.1077, 0.1524, 0.1722, 0.1308],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "S = space()\n",
        "print(S)\n",
        "model(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwIKOe-dagoi",
        "outputId": "6055cd34-f9ea-4c03-b139-0399203b49f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15.620139122009277\n"
          ]
        }
      ],
      "source": [
        "time_1 = time.time()\n",
        "\n",
        "\n",
        "alpha = 0.9\n",
        "max_ep = 1000\n",
        "\n",
        "for episode in range(max_ep):\n",
        "    SPACE = space()\n",
        "    if episode%2:\n",
        "        SPACE = bolvan(SPACE)\n",
        "\n",
        "    while SPACE.sum() + DL:\n",
        "\n",
        "\n",
        "        probs = model(SPACE)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        reward = SPACE[action]\n",
        "        loss = -alpha*reward*m.log_prob(action)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        SPACE[action] = -1.\n",
        "\n",
        "        SPACE = bolvan(SPACE)\n",
        "\n",
        "\n",
        "print(time.time() - time_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRK2alqXbL3s",
        "outputId": "f7504ae7-e0f4-4313-f2b2-3adc9382585b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.17707204818725586\n",
            "tensor(33.2100)\n"
          ]
        }
      ],
      "source": [
        "time_1 = time.time()\n",
        "\n",
        "max_ep = 100\n",
        "R = 0\n",
        "for episode in range(max_ep):\n",
        "    SPACE = space()\n",
        "    if episode%2:\n",
        "        SPACE = bolvan(SPACE)\n",
        "\n",
        "    while SPACE.sum() + DL:\n",
        "\n",
        "\n",
        "        probs = model(SPACE)\n",
        "        action = probs.argmax()\n",
        "        reward = SPACE[action]\n",
        "        R += reward\n",
        "\n",
        "\n",
        "        SPACE[action] = -1.\n",
        "\n",
        "        SPACE = bolvan(SPACE)\n",
        "\n",
        "\n",
        "print(time.time() - time_1)\n",
        "print(R/max_ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtxJhjfWb0YQ"
      },
      "source": [
        "$$SoftMax(x_1,x_2,\\dots,x_n)=\\left( \\frac{e^{x_1}}{\\sum\\limits_{i=1}^{n}e^{x_i}}, \\frac{e^{x_2}}{\\sum\\limits_{i=1}^{n}e^{x_i}},\\dots, \\frac{e^{x_n}}{\\sum\\limits_{i=1}^{n}e^{x_i}}\\right) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "HxwsNu5Kb39B"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x)/(np.exp(x).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxQ8qcjOcH7t",
        "outputId": "b22a0103-20ea-4d57-e39f-8525ebf6f014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.01321289, 0.26538793, 0.72139918])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax([-1,2,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Y_4Iy2cZmz",
        "outputId": "d17499f7-fe7e-4f22-d430-205691b6d267"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kuzin\\AppData\\Local\\Temp\\ipykernel_10284\\1649720306.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  torch.nn.functional.softmax(torch.tensor([-1.,2.,3.]))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([0.0132, 0.2654, 0.7214])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.softmax(torch.tensor([-1.,2.,3.]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d71e8b1932faf56dee17752329eeb73c746bdacdb1acbdc067bcd5bd3a88241"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
